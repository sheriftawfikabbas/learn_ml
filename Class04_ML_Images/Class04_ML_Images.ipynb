{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malaria parasite detection using ensemble learning in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Loading the cell image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning combines the predictions of multiple models to improve the prediction accuracy.\n",
    "\n",
    "There are several ways to perform ensemble learning, and a reasonable summary is available on https://en.wikipedia.org/wiki/Ensemble_learning. Simply speaking, there are two major classes of ensemble learning:\n",
    "- Bagging: fit independent models and then average their predictions\n",
    "- Boosting: fit several models sequencially and then average their predictions\n",
    "\n",
    "In this project we will be using a simplified form of the bagging approach:\n",
    "\n",
    "- fit a collection of independent models\n",
    "- gather prediction from each\n",
    "- apply a voting procedure\n",
    "\n",
    "Given the predictions from several models, there are two voting procedures\n",
    "- Hard voting: get the most common class predicted\n",
    "- Soft voting: get the argmax of the sum of predicted probabilities. It can be either weighted or unweighted.\n",
    "  - Weighted: each predicted probability is multiplied by a preset weight\n",
    "  - Unweighted: we don't multiply the predicted probabilities, we just add them straight away\n",
    "\n",
    "In the interest of time, we will focus on hard voting.\n",
    "\n",
    "First we import the required libraries: tensorflow, keras, sklearn, cv2, matplotlib, statistics and a few other utilities.\n",
    "\n",
    "Dataset: https://www.tensorflow.org/datasets/catalog/malaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keras tensorflow sklearn matplotlib opencv-python pandas\n",
    "\n",
    "import statistics\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent import futures\n",
    "import threading\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Conv2D, Activation, Dense, MaxPooling2D, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data. \n",
    "- File names are obtained using the glob module.\n",
    "- Create a data frame object for infected and healthy cell images\n",
    "- Randomize the order of data\n",
    "- Pick the first 2000 images\n",
    "- Check how many of each class are there in the sample; we should be close to 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infected = 'cell_images/Parasitized'\n",
    "healthy = 'cell_images/Uninfected'\n",
    "\n",
    "infected_files = glob.glob(infected+'/*.png')\n",
    "healthy_files = glob.glob(healthy+'/*.png')\n",
    "\n",
    "files_df = pd.DataFrame({\n",
    "    'img': infected_files + healthy_files,\n",
    "    'malaria': [1] * len(infected_files) + [0] * len(healthy_files)\n",
    "})\n",
    "\n",
    "files_df = files_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Just to reduce complexity\n",
    "files_df = files_df.iloc[0:2000, :]\n",
    "files_df['malaria'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Transform the image files into arrays and create the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image files, as they are, are binary. We need to turn them into numbers so we can pass them into the machine learning pipeline.\n",
    "\n",
    "To do so, we will use the cv2 library to read and resize the images. These operations will be performed by the `get_data()` function.\n",
    "\n",
    "Next, we place the input arrays into `X` and target values into `y`. We have to normalize the image data by dividing all `X` values by 255, so numbers would range from 0 to 1.\n",
    "\n",
    "Now that our `X` and `y` are ready, we split the dataset into 80:20 train:test split.\n",
    "\n",
    "Finally, let's see how the image will look like: use the `imshow()` function in matplotlib, which plots images from 3-d arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_length, img_width = 50, 50\n",
    "\n",
    "\n",
    "def get_data(data_files):\n",
    "    data = []\n",
    "    for img in data_files:\n",
    "        print(img)\n",
    "        img = cv2.imread(img)\n",
    "        img = cv2.resize(img, dsize=(img_length, img_width),\n",
    "                         interpolation=cv2.INTER_CUBIC)\n",
    "        img = np.array(img)\n",
    "        data += [img]\n",
    "    return np.array(data)\n",
    "\n",
    "X = files_df['img'].values\n",
    "y = files_df['malaria'].values\n",
    "\n",
    "X_converted = get_data(X)/255.0\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    X_converted, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check images\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(train_data[0])\n",
    "plt.title('{}'.format(train_labels[0]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig('sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create a deep CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to start doing deep learning to predict the presence or absence of malaria in cell images.\n",
    "\n",
    "We will be experimenting with a deep convolutional neural network which has the following architecture:\n",
    "\n",
    "- two 32 convolutional layers, each followed by max pooling\n",
    "- 64 convolutional layer, followed by max pooling\n",
    "- layer flattening\n",
    "- a dense hidden layer with 64 nodes\n",
    "- dropping 50% of the prev hidden layer\n",
    "- output layer with 1 node\n",
    "\n",
    "The Adam optimizer will be used with a learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                 input_shape=(img_length, img_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Train and test the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the fit method to optimize the model in 25 epochs, then perform prediction using the predict_classes method.\n",
    "\n",
    "We measure our prediction accuracy using the classification_report function, which gives us the key classification metrics. I will also display those metrics individually so you can know their formulas.\n",
    "\n",
    "- Precision: ability of the classifier not to label as positive a sample that is negative.\n",
    "- Recall: the ability of the classifier to find all the positive samples\n",
    "- f1: weighted average of the precision and recall\n",
    "\n",
    "In additioin:\n",
    "- Accuracy: measures how close the predicions are to the actual values\n",
    "\n",
    "Using the history object, we plot the validation accuracy and loss across the epochs to see how our models coverged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data, y=train_labels, batch_size=64, epochs=20,\n",
    "                    verbose=1, shuffle=True, validation_data=(val_data, val_labels))\n",
    "\n",
    "y_predicted = model.predict_classes(val_data)\n",
    "\n",
    "# accuracy = (true positives + true negatives) / (positives + negatives)\n",
    "print('Accuracy: ', accuracy_score(val_labels, y_predicted))\n",
    "# precision = true positives / (true positives + false positives)\n",
    "print('Precision: ', precision_score(val_labels, y_predicted))\n",
    "# recall = true positives / (true positives + false negatives)\n",
    "print('Recall: ', recall_score(val_labels, y_predicted))\n",
    "# f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('f1: ', f1_score(val_labels, y_predicted))\n",
    "\n",
    "print(classification_report(val_labels, y_predicted))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the CNN models ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know we can achieve good accuracy with one CN model, let's try an ensemble of CNN models. Let's generate an ensemble of 2 more models using a formula, as in this code.\n",
    "\n",
    "Here we create the models and place them in a dictionary of models, `models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for j in range(2, 4):\n",
    "    newmodel = Sequential()\n",
    "    newmodel.add(Conv2D(j*16, (3, 3), activation='relu',\n",
    "                        input_shape=(img_length, img_width, 3)))\n",
    "    newmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    newmodel.add(Conv2D(j*16, (3, 3), activation='relu'))\n",
    "    newmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    newmodel.add(Conv2D(j*32, (3, 3), activation='relu'))\n",
    "    newmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    newmodel.add(Flatten())\n",
    "    newmodel.add(Dense(j*32, activation='relu'))\n",
    "    newmodel.add(Dropout(0.5))\n",
    "    newmodel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    newmodel.compile(optimizer=adam,\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "    newmodel.summary()\n",
    "    models[j] = newmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Fit the models in the ensemble and perform the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit each of the models separately using the same datasets.\n",
    "\n",
    "Once done, we generate the predictions and add them into an array, `predictions_hard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in models:\n",
    "    models[j].fit(x=train_data, y=train_labels, batch_size=64, epochs=20,\n",
    "                  verbose=1, shuffle=True, validation_data=(val_data, val_labels))\n",
    "\n",
    "\n",
    "models[1] = model\n",
    "\n",
    "predictions_hard = []\n",
    "for j in models:\n",
    "    predictions_hard += [models[j].predict_classes(val_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Apply hard voting to the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will apply the hard voting procedure. This is by deciding the class according the majority vote. We will use the `mean()` statistical function to get the class that was predicted more frequently than the other.\n",
    "\n",
    "Remember that the mean function gives you the data value that was repeated the most in the dataset. So in our predicted classes, where there are only 1's and 0's, it will pick the value that was repeated most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_hard = []\n",
    "for i in range(0, len(val_data)):\n",
    "    voting_hard += [statistics.mode(\n",
    "        [predictions_hard[0][i][0], predictions_hard[1][i][0], predictions_hard[2][i][0]])]\n",
    "\n",
    "print(classification_report(val_labels, voting_hard))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
