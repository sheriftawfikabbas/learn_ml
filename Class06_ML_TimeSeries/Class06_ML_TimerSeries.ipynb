{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing time series predictions of COVID-19 deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Comparison](comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Understand the basics of time series analysis\n",
    "\n",
    "- What's a time series?\n",
    "  - It's a table of values, such as temperature or stock price, that are observed at regular times, such as every hour or every day.\n",
    "  - Here is a simple time series: temperatures in Celcius in The Neighborhood over 10 days.\n",
    "\n",
    "| Day         | Temperature |\n",
    "| ----------- | ----------- |\n",
    "| 11 May 2022 | 23          |\n",
    "| 12 May 2022 | 21          |\n",
    "| 13 May 2022 | 24          |\n",
    "| 14 May 2022 | 19          |\n",
    "| 15 May 2022 | 18          |\n",
    "| 16 May 2022 | 20          |\n",
    "| 17 May 2022 | 21          |\n",
    "| 18 May 2022 | 22          |\n",
    "| 19 May 2022 | 20          |\n",
    "| 20 May 2022 | 21          |\n",
    "\n",
    "- One would want to know: what's the temperature after 20 May 2022? Predicting the data values (temperature) in the future is called forecasting.\n",
    "- Workflow for time series analysis:\n",
    "  - Step 1: Underst the data (Task 2)\n",
    "  - Step 2: Identify the right model (Tasks 3, 4, 5, 6, 7)\n",
    "  - Step 3: Use the best model to forecast future values (Task 8)\n",
    "\n",
    "- To make the best out of this hands-on project, we need to learn a few concepts before the hands-on tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common features in time series data\n",
    "\n",
    "\n",
    "![Trends and seasons](trends_seasons.png)\n",
    "\n",
    "- These are three common features in every time series data:\n",
    "    - Seasons: The regular ups and downs in your data, like in the figure above.\n",
    "    - Trends: When you find that data is general going upward or downword. In the figure above, there is an upward trend.\n",
    "    - Cycles: Where there are ups and downs in the data that do not seem to repeat regularly.\n",
    "- We can observe the above three features by plotting the data, and checking it visually.\n",
    "\n",
    "## Stationarity\n",
    "\n",
    "- A stationary time series: it's the time series in which the average and variance do not change; so these statistical properties are *independent of time*. \n",
    "- Therefore, a time series with trends is not stationary. An example of such data is in the figure above.\n",
    "- Non-stationary data are not easy to predict, but stationary data are much easier to predict.\n",
    "- Therefore, we need to make our time series data stationary before we feed them into statistical models.\n",
    "- You will learn how to turn non-stationary data into stationary data by using differencing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "We obtain the dataset from the Github page of John Hopkins Center for System Science and Engineering:\n",
    "\n",
    "[https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Explore and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly, add_changepoints_to_plot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_df = pd.read_csv('time_series_covid19_deaths_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the data for the daily deaths in all countries in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how the data looks like, by using the head() method from the DataFrame class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the columns are arranged as follows: <br>\n",
    "<ul>\n",
    "    <li>\n",
    "        feature columns such as Province, Country etc. </li>\n",
    "    <li>date columns, starting from the first reporting date, 1/22/2020, until two days ago (with respect to the time of recording this course</li>\n",
    "   </ul>\n",
    " \n",
    "The date columns start from 1/22/20, so let's take the data in these columns separately.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = deaths_df.loc[:, '1/22/20':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sum row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the numeric values here, so we convert `d` to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new data frame with two columns. This will be our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(columns=['ds', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dates from the columns in `deaths_df` data frame, starting from the fifth column. They will be obtained in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(deaths_df.columns[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the string dates into the datetime format using the `to_datetime()` method, so that we can perform datetime operations on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(pd.to_datetime(dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assign the dates and deaths data to the columns in the new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['ds'] = dates\n",
    "dataset['y'] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to have only 1 data column, y, with the index being `ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.set_index('ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the daily number of deaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(dataset)\n",
    "plt.savefig('cummulative_daily_deaths', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase has been tremendous, but it seems to start to plateau. Maybe too early to know? I'd leave that discussion to the scientists in charge.<br>\n",
    "\n",
    "This time series is obvious non-stationary, and we cannot observe any seasonal behavior here. Let's perform seasonal decomposition of the data by using the `seasonal_decompose()` function from `statsmodel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "seas_d=sm.tsa.seasonal_decompose(dataset,model='add');\n",
    "fig=seas_d.plot()\n",
    "fig.set_figheight(4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is both seasonal and trending upwords. We need the data to be stationary so that we can apply our models to it.\n",
    "\n",
    "Let's make our dataset stationary by taking the difference between consecutive elements, which in our case will be the daily change in the number of deaths. To do this, we will use the `diff()` method from the `dataset` DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(dataset.diff())\n",
    "plt.savefig('daily_deaths_diff', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sample = dataset.loc['2020-11-01':'2021-01-01']\n",
    "plt.plot(sample.diff())\n",
    "plt.xticks(sample.index,rotation=90)\n",
    "plt.savefig('daily_deaths_diff_sample', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the period length - it's 7 days! The seasonality of deaths is therefore weekly. But there is still an upword trend in the data.\n",
    "\n",
    "Therefore, it is not yet stationary, so let's take the `diff()` one more time. Doing that is what is known as *second-order differencing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(dataset.diff().diff())\n",
    "plt.savefig('daily_deaths_2nd-order_diff', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly stationary enough, and we will be using this for our subsequent analysis.\n",
    "\n",
    "The second-order differencing is essentially the difference of the difference, or the change in the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_diff = dataset.diff().diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove the first two data point here, which will be two `None` values after applying `diff()` twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_diff = dataset_diff.loc['2020-01-24':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in this task is to split our dataset into a training and a test set. \n",
    "- When we train ML models, we typically use 20% of the dataset as a test set.\n",
    "- In time series analysis, the the size of the test set should be close to the extent of the future data.\n",
    "- In this exercise, we are only going to predict 1 month into the future. Therefore, we will use all the data before 15 April 2022 as training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = '2022-04-15'\n",
    "\n",
    "print(cutoff_date)\n",
    "\n",
    "train = dataset_diff.loc[dataset_diff.index < pd.to_datetime(cutoff_date)]\n",
    "test = dataset_diff.loc[dataset_diff.index >= pd.to_datetime(cutoff_date)]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(train)\n",
    "plt.savefig('training_set', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Forcasting using SARIMAX, or Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMAX is one of the time series models in the python statistics library `statsmodels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMAX uses two sets of parameters:\n",
    "- The `order`: a tuple of values `p`, `q` and `d`. They control the number of parameters in the model. Here, let's use p=2, q=1 and d=3.\n",
    "- The `seasonal_order`: a tuple of values `P`, `D`, `Q` and `s`. They control the seasonal component of the model, and `s` is the periodicity of the dates. So for example, weekly periodicity can be set with `s=7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SARIMAX(train, order=(2, 1, 3), seasonal_order=(0,0,0,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the fit method to optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions using the model, and compare those against the values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_prediction = results.predict(\n",
    "    start=cutoff_date, end='2022-05-15', dynamic=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "l1, = plt.plot(dataset_diff, label='Observation')\n",
    "l2, = plt.plot(sarimax_prediction, label='ARIMA')\n",
    "plt.legend(handles=[l1, l2])\n",
    "plt.savefig('SARIMAX_prediction', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since that we are interested in comparing between the different time series analysis approaches, we are going to use one of the validation measures: mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SARIMAX MAE = ', mean_absolute_error(sarimax_prediction, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in a typical machine learning workflow, we should find the best values of p, q and r that will minimize the error. We can use the auto_arima function in the pmdarima module to do that. This will find the optimal parameter combintation and return the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.auto_arima(train, start_p=1, start_q=1,\n",
    "                      m=7,\n",
    "                      d=None,\n",
    "                      seasonal=True,\n",
    "                      start_P=0,\n",
    "                      D=0,\n",
    "                      trace=True,\n",
    "                      error_action='ignore',\n",
    "                      suppress_warnings=True,\n",
    "                      stepwise=True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this gives us the best mode here: \"Best model:  SARIMAX(3,0,2)(2,0,1)[7]\". Now let's fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sarimax_prediction_atutomated = model.predict(n_periods=test.shape[0])\n",
    "sarimax_prediction_atutomated = pd.DataFrame({'ds':test.index,'y':sarimax_prediction_atutomated})\n",
    "sarimax_prediction_atutomated.set_index('ds',inplace=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "l1, = plt.plot(dataset_diff, label='Observation')\n",
    "l2, = plt.plot(sarimax_prediction, label='ARIMA')\n",
    "plt.legend(handles=[l1, l2])\n",
    "plt.savefig('SARIMAX_prediction_automated', bbox_inches='tight', transparent=False)\n",
    "print('SARIMAX auto MAE = ', mean_absolute_error(sarimax_prediction_atutomated, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops: the model that is selected by the `auto_arima()` method is doing worse in terms of MAE! \n",
    "\n",
    "This is related to a long discussion on: *which accuracy measure to use when we compare time series models?*\n",
    "\n",
    "We will not have time here to delve into it, so we will just choose the model that has the lowest MAE.\n",
    "\n",
    "Let's run the standard diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The diagnostic plots enable us to analyse the accuracy of our model, and whether there is any (residual) information in the data that should be used to improve the model.\n",
    "- The package provides four diagnostic plots. Below I only give a very brief description of these plots.\n",
    "  - Standardized residuals over time plot: Calculates the residue, or the difference between observed and predicted values, as a function of time.\n",
    "  - Histogram: It's the count of values against the computed residual. Note: that residual in the x-axis was the one in the y-axis in the plot above.\n",
    "  - Normal Q-Q: Typically, model errors should be normally distributed. This plot checks if this is the case; if the points are nearly linear, then the errors are normally distributed. Which is the case in our plot.\n",
    "  - Correlogram: Checks the autocorrelation in the data, to ensure that the data is random. \n",
    "- *Randomness of data* is a necessary condition for the *validity* of the model, and it is checked by the correlogram.\n",
    "- In the correlogram, the *autocorrelation* is plotted against *time lags*. If the values are close to zero, then this is an indication of the randomness of the data, as is the case in our plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Forcasting using Facebook's Prophet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the open source time series library released by Facebook. It is also widely used by Facebook in their own time series analysis tasks. Facebook prophet does not require that you specify or search for hyperparameters. The model can act as a black box that does all the required computations on its own. And it works with the same object-fit-predict API.\n",
    "\n",
    "Prophet expects the data frame to have 2 columns, unlike SARIMAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ds'] = train.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a new Prophet object and call the `fit()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "m.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's use the model to generate the predictions for the test set.\n",
    "- First we create the `future` data frame using `make_future_dataframe()`, then we call the `predict()` function.\n",
    "- In `make_future_dataframe()`, we pass the `periods` parameter, which is the number of days we want for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=test.shape[0])\n",
    "prophet_prediction = m.predict(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us calculate the mean absolute error for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_prediction = prophet_prediction.set_index('ds')\n",
    "prophet_future = prophet_prediction.yhat.loc[prophet_prediction.index >= cutoff_date]\n",
    "print('Prophet MAE = ', mean_absolute_error(prophet_future, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "l1, = plt.plot(dataset_diff, label='Observation')\n",
    "l1, = plt.plot(prophet_future, label='Prophet')\n",
    "plt.legend(handles=[l1, l2])\n",
    "plt.savefig('prophet predictions',\n",
    "            bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Preparing the dataset for XGBOOST and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the prophet and SAIMAX models, the two models we will train in Task 6, namely XGBOOST and NN, are supervised machine learning models that deal with independent data points, or examples. It assumes that each data point is totally independent from the rest of the data points in the dataset.\n",
    "\n",
    "Here is a method that extracts these features from a given dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(t):\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    X['day'] = t.index.day\n",
    "    X['month'] = t.index.month\n",
    "    X['quarter'] = t.index.quarter\n",
    "    X['dayofweek'] = t.index.dayofweek\n",
    "    X['dayofyear'] = t.index.dayofyear\n",
    "    X['weekofyear'] = t.index.weekofyear\n",
    "    y = t.y\n",
    "    return X, y\n",
    "\n",
    "\n",
    "featurize(dataset_diff)[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Create training and test datasets by splitting the dataset, and perform data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we are going to cover two topics in data preparation: splitting the dataset into training and test data, and normalizing the data.\n",
    "\n",
    "You might have already performed a splitting operation of a dataset in machine learning, where one takes a randomly selected portion of the dataset, say 20%, as a test set, while the remaining 80% is the training set. It is randomly selected because the whole dataset is randomly shufflled before the selection. Another popular approach is the k-fold cross validation.\n",
    "\n",
    "However, those two methods won't work with time series data. The reason is: when we train the model on the training set, the purpose is to predict the target values in the future, which corresponds to date values that are outside of the date values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = featurize(\n",
    "    dataset_diff.loc[dataset_diff.index < pd.to_datetime(cutoff_date)])\n",
    "X_test, y_test = featurize(\n",
    "    dataset_diff.loc[dataset_diff.index >= pd.to_datetime(cutoff_date)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's discuss data normalization. We perform data normalization so as to make the range of values of the features, or the columns in the X_train table, as close as possible. For example, we have the features dayofweek and dayofyear. The range of values of dayofweek is from 1 to 7, whereas dayofyear is from 1 to 365. Having such large differences in the ranges of values will either slow down the training of the machine learning model or make it quite difficult. We solve this problem by applying normalization. There are several ways we can normalize the data with. Here I will choose the StandardScaler, which applies the following equation on each of the columns.\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "Here x is the column before scaling, u is thee mean and s is the standard deviation. So basically, we subtract the mean of each column from itself, then divide by the standard deviation of that column. To apply StandardScaler, we first fit the scaler object to the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the scaling to both the training and test sets, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = scaler.transform(X_train)\n",
    "scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Train the XGBOOST and NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the XGBRegressor object which will represent the XGBOOST regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBOOST_model = XGBRegressor(n_estimators=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the XGBOOST regression model using the fit method, and perform prediction using the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBOOST_model.fit(scaled_train, y_train,\n",
    "                  eval_set=[(scaled_train, y_train), (scaled_test, y_test)],\n",
    "                  verbose=True)\n",
    "XGBOOST_prediction = XGBOOST_model.predict(scaled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the mean absolute error for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGBOOST MAE = ', mean_absolute_error(XGBOOST_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation and training of the feedforward neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "NN_model.add(Dense(20, input_shape=(scaled_train.shape[1],)))\n",
    "NN_model.add(Dense(10))\n",
    "NN_model.add(Dense(1))\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer=Adam(lr=0.001))\n",
    "NN_model.fit(scaled_train, y_train, validation_data=(\n",
    "    scaled_test, y_test), epochs=210, verbose=1)\n",
    "NN_prediction = NN_model.predict(scaled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the MAE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGBOOST MAE = ', mean_absolute_error(XGBOOST_prediction, y_test))\n",
    "print('Prophet MAE = ', mean_absolute_error(prophet_future, test))\n",
    "print('SARIMAX MAE = ', mean_absolute_error(sarimax_prediction, test))\n",
    "print('NN MAE = ', mean_absolute_error(NN_prediction, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The peformance of SARIMAX is the lowest, while that of XGBOOST is the highest. <br>\n",
    "Finally let us visualize the predictions of all 4 models. Note the autofmt_xdate method in matplotlib, it knows how to appropriately rotate the date labels on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XGBOOST_df = pd.DataFrame({'y': XGBOOST_prediction.tolist()})\n",
    "XGBOOST_df.index = y_test.index\n",
    "\n",
    "NN_df = pd.DataFrame(NN_prediction)\n",
    "NN_df.index = y_test.index\n",
    "plt.figure(figsize=(20, 20))\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.suptitle('Compare SARIMAX, prophet, XGBOOST and NN')\n",
    "axs[0, 0].plot(dataset_diff.tail(50))\n",
    "axs[0, 0].plot(sarimax_prediction.tail(50))\n",
    "axs[0, 0].set_title(\"SARIMAX\")\n",
    "axs[0, 1].plot(dataset_diff.tail(50))\n",
    "axs[0, 1].plot(prophet_future.tail(50))\n",
    "axs[0, 1].set_title(\"Prophet\")\n",
    "axs[1, 0].plot(dataset_diff.tail(50))\n",
    "axs[1, 0].plot(XGBOOST_df.tail(50))\n",
    "axs[1, 0].set_title(\"XGBOOST\")\n",
    "axs[1, 1].plot(dataset_diff.tail(50))\n",
    "axs[1, 1].plot(NN_df.tail(50))\n",
    "axs[1, 1].set_title(\"NN\")\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.savefig('comparison',\n",
    "            bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Forecast the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have established the XGBOOST model is the most accurate, and therefore we can use it to forcast the future COVID19 deaths i.e. beyond 15th May 2022.\n",
    "- For the sake of exercise, we will apply all four models for the forecast.\n",
    "- We will ask the four models to forecast 1 month into the future: from 16th May 2022 to 16th June 2022.\n",
    "- Prediction with SARIMAX and fbprophet is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_future_prediction = results.predict(start='2020-01-24', end='2022-06-15', dynamic=False)\n",
    "future = m.make_future_dataframe(periods=test.shape[0] + 31)\n",
    "prophet_future_prediction = m.predict(future)\n",
    "prophet_future_prediction = prophet_future_prediction.set_index('ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For XGBOOST and NN, we need to generate the `X` dataset as we did before. We need to create a new function like `featurize()`, which takes a DataFrame of dates only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_dates(t):\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    X['day'] = t.index.day\n",
    "    X['month'] = t.index.month\n",
    "    X['quarter'] = t.index.quarter\n",
    "    X['dayofweek'] = t.index.dayofweek\n",
    "    X['dayofyear'] = t.index.dayofyear\n",
    "    X['weekofyear'] = t.index.weekofyear\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it on the `future` DataFrame we got from fbprophet to generate forecasts using the trained XGBOOST model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = future.set_index('ds')\n",
    "X_future = featurize_dates(future)\n",
    "scaled_future = scaler.transform(X_future)\n",
    "XGBOOST_future = XGBOOST_model.predict(scaled_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_future = NN_model.predict(scaled_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the entire time series including training, test and future predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XGBOOST_df = pd.DataFrame({'y': XGBOOST_future.tolist()})\n",
    "XGBOOST_df.index = future.index\n",
    "\n",
    "future_prediction_count = 31 + test.shape[0]\n",
    "\n",
    "NN_df = pd.DataFrame(NN_future)\n",
    "NN_df.index = future.index\n",
    "plt.figure(figsize=(20, 20))\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.suptitle('Compare forecasts of SARIMAX, prophet, XGBOOST and NN')\n",
    "axs[0, 0].plot(dataset_diff.tail(60))\n",
    "axs[0, 0].plot(sarimax_future_prediction.tail(60))\n",
    "axs[0, 0].set_title(\"SARIMAX\")\n",
    "axs[0, 1].plot(dataset_diff.tail(60))\n",
    "axs[0, 1].plot(prophet_future_prediction['yhat'].tail(60))\n",
    "axs[0, 1].set_title(\"Prophet\")\n",
    "axs[1, 0].plot(dataset_diff.tail(60))\n",
    "axs[1, 0].plot(XGBOOST_df.tail(60))\n",
    "axs[1, 0].set_title(\"XGBOOST\")\n",
    "axs[1, 1].plot(dataset_diff.tail(60))\n",
    "axs[1, 1].plot(NN_df.tail(60))\n",
    "axs[1, 1].set_title(\"NN\")\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.savefig('comparison_forecasts',\n",
    "            bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all, I hope you found this tutorial useful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
